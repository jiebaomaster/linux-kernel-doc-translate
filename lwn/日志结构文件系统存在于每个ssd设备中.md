# 日志结构文件系统存在于每个 SSD 设备中

> 原文链接 [Log-structured file systems: There's one in every SSD](https://lwn.net/Articles/353411/)，by Valerie Aurora，2009-11-18

当说到“日志结构文件系统”时，大多数存储开发人员会立即想到 Ousterhout  和 Rosenblum 的经典论文 [The Design and Implementation of a Log-structured File System](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.8933) 和接下去二十年来在尝试解决令人讨厌的垃圾回收问题的工作（请参阅下文）；Linux 开发人员可能会想到 JFFS2，NILFS 或 LogFS，这三个是专门用于固态设备（SSD）的现代日志结构文件系统中；然而，很少有人会想到 SSD 固件（firmware）。现代功能完善的 SSD 中存在一个在许多重要方面都和日志结构文件系统类似的闪存转换层（flash translation layer，FTL）。通过对日志结构文件系统的研究，使我们知道如何获得 SSD 的最佳性能。特别的，在 SSD 和文件系统层级上全面支持 [TRIM 命令](https://lwn.net/Articles/293658/) 对于保证 SSD 能够长期运行在峰值性能下至关重要。

## 日志结构文件系统是什么？

奇怪的是，日志结构文件系统是从日志文件系统演变而来的。日志（logging or journaling）文件系统可以理解为在 ext2 或 FFS 风格的常规就地写（write-in-place）文件系统基础上，添加了写操作日志功能。日志可以保证磁盘数据的状态一致性，在写操作前，先将操作的摘要写到日志，并存储在非易失性的位置，例如磁盘或更快更贵的 NVRAM。因为日志记录包含足够的信息，当对文件系统的实际写操作由于系统奔溃（system crash）等原因中途中断了，可以通过日志重复整个操作。此恢复过程称为重播日志。可以看到，对文件系统的每次更改都会有两次磁盘写入：一次写入日志，一次写入永久更改。

1988年左右，Ousterhout 和合作者意识到，如果将整个文件系统视为一个巨大的日志，则可以完全跳过第二次写操作。与其先写操作日志，然后写实际更改到磁盘上其他位置，不如将操作一次写入日志末尾（无论在何处），然后对其进行处理。对现有文件和 inode 的更改采用写时复制（copy-on-write）模式：旧版本被标记为可用空间，新版本被写入日志的末尾。从概念上讲，查找文件系统的当前状态需要从头到尾重播日志。实际上，日志结构文件系统拥有描述该时间点文件系统状态的检查点，检查点会被定期写入磁盘。这样就可以通过检查点来查看状态，而无需日志重播。通过重播检查点之后相对较少数量的日志，可以恢复检查点之后对文件系统的任何更改。

日志结构文件系统（LFS）结构的有趣好处之一是，对文件系统的大多数写入都是顺序的。Sprite LFS 的开发动机表面了20年来存储领域几乎没有什么变化：
> 在过去的十年中，CPU速度得到了极大的提高，而磁盘访问时间的改善却很慢。这种趋势将来可能会继续，它将导致越来越多的应用程序受限于磁盘访问时间（disk-bound）。[...] 日志结构文件系统基于以下假设：文件被缓存在内存中，并且不断增加的内存大小将使缓存在满足读取请求方面越来越有效。因此，磁盘流量将由写操作决定。

等等，为什么我们仍在谈论磁盘搜索？SSD彻底改变了存储的性能特征！磁盘已死！闪存万岁！

相比传统存储介质，日志结构文件系统出乎意料地适合于 SSD。日志结构文件系统的基本假设是：读取便宜，而写入昂贵，这与 SSD 中[基于 NAND 的闪存](https://en.wikipedia.org/wiki/Flash_memory#NAND_flash)储存块正好一致。下文中，“闪存”指基于 NAND 的裸闪存，而 SSD 指的是具有损耗均衡（wear-leveling），写收集（write-gathering）闪存转换层的基于 NAND 的闪存设备。闪存的读取操作粒度为几百个字节，但是写操作需要以数万或数十万个字节的连续块大粒度执行。闪存写入分为两个步骤：首先清除整个块，将所有位设置为相同的值（与直觉相反，通常是置1）， 然后，将目标块中的各个位按需翻转回0，直到得到你想要写入的数据。

日志结构文件系统天生适合闪存。日志结构设计的细节之一是，日志以大的连续块，​​称为“段（segments）”写入，大小约为几兆字节。为了减少元数据（metadata）开销并获得最佳性能，多条日志将被收集一起顺序写出到完全空闲的段中。在任意时间点，大多数段处于部分使用和部分空闲状态，因此文件系统在开始对其进行写入之前，必须从该段收集所有正在使用的数据，移动到其他位置，空出整段来才能进行写入。当文件系统需要一个新的段时，它首先通过将所有正在使用的数据或活动数据移动到另一个空闲段中来清理现有的部分使用的段。该过程称为垃圾收集（garbage-collects）。一切都安排好之后，文件系统就可以对空段进行一次大的流式写入。鉴于闪存必须在写入之前擦除大型连续的块，因此日志结构文件系统的这种分段和清理模式正是有效写入闪存设备所需要的。

当你查看为裸闪存接口编写的文件系统时，日志结构文件系统与闪存之间的匹配显而易见。对于管理擦除块和闪存硬件其他细节的文件系统，总是被设计为日志结构。Linux 支持的此类文件系统中最广泛使用的是 JFFS2，它用于许多嵌入式设备，例如售票机和后座航空娱乐系统。我不止一次登上飞机，看到一条 JFFS2 错误消息，报告悬挂的座椅靠背娱乐系统上的闪光灯损坏。（可悲的是，现在可能有成千上万的人将 Tux 企鹅启动徽标与无法在长途飞行中看电视相关联。）

对于支持磁盘风格块设备接口的 SSD，如今大多数消费者级别的 SSD 都这么做，操作系统使用常规文件系统通过块接口与 SSD 进行通讯，例如：将第37号块读入此缓冲区，将缓存写入到第42块。然而，在 SSD 内部，存在着一套包含日志结构文件系统等效逻辑的系统。这套系统要解决与日志结构文件系统相同的问题，如实现损耗平衡，写收集。

大多数 SSD 制造商拒绝透露其内部固件的任何细节，但我们可以完全确定它与日志结构文件系统有很多共同点。首先，实现高效随机写入的唯一方法是对它们进行缓存，然后将它们一起写到单个擦除块中。这需要清除擦除块，将所有使用中的块移动到另一个区域，并在块的逻辑位置与其物理位置之间保持映射关系，这正是日志结构文件系统的功能。其次，当我们从[研究出版物](http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1010143)中获得 SSD 实现细节时，看起来就像是日志结构文件系统。第三，在对 SSD 的长期性能测试中，我们看到与日志结构文件系统相同的性能随时间下降的模式。我们将在下一节中详细讨论这一点。

## 日志结构文件系统的性能

日志结构文件系统很适合今天的基于闪存的存储，但是回到1990年，它似乎在磁盘存储上也有巨大的潜力。然而，当今我们并没有在基于磁盘的笔记本电脑和服务器上使用日志结构文件系统。这是为什么？

简而言之，只要大多数段清理（segment cleaning 将活动数据移出段以便可以重复使用）可以在文件系统空闲时在后台完成，日志结构文件系统的性能就相对较好。[关于LFS的第一篇主要后续论文](http://www.eecs.harvard.edu/~margo/papers/usenix93/paper.pdf)发现，在磁盘利用率，内存与磁盘比率以及文件系统流量（disk utilization, memory-to-disk ratio, file system traffic）在实际使用水平下时，LFS 的性能比最佳情况降低了40％。在稳定状态下，LFS 花费大量的磁盘访问时间清理段-将旧数据移出段，以便用于后续写入。这个段清理问题至少在另一个十年中一直是活跃的研究对象，但仍没有一种解决方案能够在实际的磁盘使用水平上一直击败现有的就地写文件系统。这有点像在内存管理中比较垃圾回收与显式引用计数两种策略。当内存使用率较低时，偶尔的高延迟是可以接受的，此时垃圾回收的便利性超过了性能优势；但是，在磁盘利用率达到 50％ 以上的“高”水平时，段清理开销和等待释放空间的周期性高延迟问题就不可接受了。

正如第一篇[LFS论文](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.8933)所展示的，想要日志结构文件系统具有良好的性能，关键是数据放置策略，好的策略应使创建空闲段的速度跟上使用的速度。LFS 的写带宽受限于产生空闲段的速率。最坏的情况是，文件系统的空间使用达到 X% 时，每个段的段空间使用也达到 X％。此时，生成一个空闲的段需要从
`N = ceiling(1/(1 - X))`
个部分使用的段中收集活跃数据，然后将老的数据写回到 N-1 个段中。举例来说，当磁盘使用率为 80% 时，要收集
`N = ceiling(1/(1- 0.8)) = 1/0.2 = 5`
个段的数据。假设每个段的大小为 1MB，且 1MB = 1000KB，则在写入 1MB 的新数据之前，我们需要先读取
`5 * (1MB * 0.8) = 5 * 800KB = 4MB`
的老数据，然后将老数据顺序写入4个段，才能开始写。

相反，最好的情况是文件系统中的段要么是完全满的要么是完全空的。最好的写模式是所有元数据和数据的更改只发生在单个段中，这样在更新数据版本时，旧版本所在段被释放，就会得到一个空闲的段。现实介于最好和最坏情况之间。日志结构文件系统的目标是使段使用情况呈现两极分化的分布特征：大多数段要么非常满，要么非常空，同时满的段往往不发生修改。事实证明这很难实现。

SSD 有一个额外的限制：损耗均衡。即便在最佳情况下，即大多数段都100％满，并且没有写入操作会更改其中的数据，SSD 仍必须偶尔移动这些段，因为它必须将写入操作分散到每个可用的闪存块上。这在某些情况下增加了额外的段移动，也导致 LFS 运行在 SSD 上比运行在磁盘上更难获得良好的性能。

## 经验教训

固态硬盘制造商可以从日志结构文件系统的20多年发展中学到很多经验。没人知道他们是否正在这样做，因为大多数制造商对 SSD 固件开发采取非常封闭的方式，这是将低利润的廉价闪存芯片转变为高利润的昂贵、可靠、高性能的存储设备的秘诀。在这件事上，[制造商们的水平显然差距很大](http://www.tomshardware.com/reviews/ssd-hdd-flash,2127-14.html)。当今，制造商采取了商业秘密策略来保持自己的竞争优势：在各个小的设计元素上申请专利，并且对整个系统的整合实现过程保密。每当文件系统开发人员向制造商询问有关 SSD 底层实现的更多信息时，他们总是说“别让你的小可爱系统程序员在这方面伤脑筋了”，“相信我们就行了”。制造商采取这种策略是有一定道理的，但这种策略有个明显的问题，它产生和强化了这样一种思维模式：不仅拒绝与外界共享信息，而且也忽略了外界的信息，例如先前发表的学术论文。

SSD 应该能从日志结构文件系统中学到很多优化的点，但是它错过了很多这样的机会，其中最大的遗憾当属 SSD 对 [TRIM 命令](http://en.wikipedia.org/wiki/TRIM_(SSD_command))的缓慢适配。TRIM 是一种对块设备的命令，通知它文件系统不再使用特定范围的块，基本可以理解为对块的 free（）操作。上文中说过，达到最佳性能需要在持续写入的同时产生空段。举一个简单的例子，当前有一个仅包含一个 inode 及其所有文件数据的段。如果下一组写入操作的覆盖了这个段中所有文件数据和 inode 数据，则在使用这个段前应该认为这一段是空闲的可用的，不必移动段中的任何活动数据到其他段中。在 SSD 中的存在类似的场景，写入已经被写入过的块。SSD 可以知道该块中的旧数据可以被释放，该块可以被立即重新使用而无需将其中的数据复制到其他位置。

但是，与还未支持 TRIM 命令的 SSD（2009年9月前生产的绝大多数商用 SSD）相比，日志结构文件系统具有明显的优势。日志结构文件系统知道磁盘上的数据是否已被释放，即便这些数据还没被覆盖。当你删除一个段文件时：整个段被释放了，但是没有发送数据覆盖。日志结构文件系统能知道文件删除发生了，现在有一个可用的空段；但是 SSD 所看到的只是对磁盘上其他块的少量写入（修改元数据），被删除文件所使用的块中的数据仍然被认为是文件系统的有效数据，这些数据会在接下去的写操作前被不停地移动。一旦设备中的每个块都至少写入了一次，SSD 便注定会进入最坏的性能状态，此时，其备用块的数量达到最少，每次旋转使用新块时都必须移动数据。

像上文所说的那样，日志结构文件系统中获得良好性能的关键是利用空闲或近乎空闲的段。不支持 TRIM 命令的 SSD 无法知道很多空闲段，这会带来极大的性能劣势，足以让所有不支持 TRIM 命令的 SSD 都感到震惊。我猜测，早期 SSD 仅在就地写入文件系统（说的就是你 NTFS）和较低的总文件系统使用率（例如，不超过70％）下进行了性能测试。

不幸的是，当前形式的 TRIM [在设计和实现上都表现得非常差](http://lwn.net/Articles/347511/)：TRIM 命令没有标记，多数 SSD 需要数百毫秒来处理 TRIM 命令。内核开发人员已经在 [Linux Plumbers 会议](http://linuxplumbersconf.org/2009/)，[Linux 存储和文件系统研讨会](Linux Storage and File System Workshop)以及邮件列表上确切讨论了如何实现 TRIM 支持，包括每个 TRIM 的性能成本是多少，TRIM 的粒度应该是多大，发送的频率是多少，以及是否可以忘记或错过 TRIM 命令。我认为，在启用了 TRIM 的设备上，块的使用/空闲状态应该像内存页面一样仔细地进行跟踪。文件系统实现可以采取显式的同步alloc（）/ free（）调用的形式，也可以采取异步垃圾回收的方式（在文件系统检查或清理运行期间），而且应该像防范内存泄露一样防范块泄露。此外，在理想情况下，TRIM 将被重新设计或替换为 ATA 规范中功能齐全，设计良好的一等公民命令，而不是在问题发生后采用的 hack 式的解决方式。

当然，所有这些都是在没有 SSD 制造商的实施细节的情况下进行的推测。也许某些 SSD 固件程序员提出了全新的算法，用于重新映射和写收集，这些算法根本不类似于日志结构的文件系统，并且到目前为止，我们所看到的性能特征和优化恰好与日志结构文件系统相匹配。然而，到目前为止将 SSD 视为由日志结构文件系统支持似乎是获得良好性能的一个很好的经验法则。SSD 和文件系统对 TRIM 命令的完整支持将是获取长期良好性能的关键。
