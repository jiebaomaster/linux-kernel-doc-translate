# 调度实体级别的负载跟踪

> 原文链接 [Per-entity load tracking, by Jonathan Corbet
January 9, 2013](https://lwn.net/Articles/531853/)

Linux 内核的 CPU 调度程序需要满足以下需求：公平，响应迅速的方式给进程分配处理器，同时最大程度地提高系统吞吐量，并最小化功耗。用户期望上述结果，不管他们自己的工作负载的特征如何，也不管这些目标事实上经常相互冲突。因此，这些年来内核经历了几个不同的 CPU 调度器就不足为奇了。随着当前的“完全公平调度器”（CFS）在 2007 年的 2.6.23 版本上被合并，调度器似乎相对稳定了。然而，表面上的稳定之下，仍然发生了很多变化，在 3.8 版本中合并的 **Per-entity load tracking** 是这一段时间以来最重大的变化之一。

完美的调度器需要一个能够预测未来的魔法水晶球。如果每个进程的执行时间和资源需求都可以被确切知道，系统就可以最佳地调度这些进程。不幸的是，硬件制造商从不提供这种预测设备，因此调度程序必须能够在缺少它们的情况下蒙混过关。所谓蒙混过关指的是”用过去 预测 未来”，而最重要的指标就是每一个进程过去的“性能”信息。但是，有趣的是，尽管内核密切跟踪每个进程的实际运行时间，但并不清楚每个进程对系统负载的贡献。

人们可能会问，“消耗的CPU时间”和“负载”之间是否有区别呢？Paul Turner 在 Per-entity load tracking 补丁集（已合并为 3.8）中回答了这个问题，他认为两者是有区别的。即使进程当前尚未实际运行，例如一个在 CPU 运行队列中等待运行的进程，也能有负载贡献。“负载”是一个瞬时量，表示当前时间点一个进程对系统的压力大小？相对的，CPU使用率是一个累积属性。一个长时间运行的进程在上周消耗了大量的处理器时间，但此刻的处理器需求可能很小。尽管过去这个进程的需求很高，但该进程现在对负载的贡献很小。

CFS 调度程序（在 3.7 和更低版本的内核中）在每个运行队列的基础上跟踪负载。值得注意的是，每个 CPU 中至少有一个 CFS 运行队列。而且，当开启组调度（group scheduling）功能时，每个控制组（control group）都有其自己的对应于每个 CPU（per-CPU）的运行队列。运行队列级别的负载跟踪可以记录每个运行队列对整个系统的负载的影响。这足以帮助组调度程序在控制组之间分配 CPU 时间。但系统并不知道当前负载的确切来源。另外即使工作负载相对稳定，运行队列级别的负载跟踪得到的负载值也有很大变化。

**更好地负载跟踪**

Per-entity load tracking 通过将跟踪推进到单个“调度实体”的级别来解决上面的问题，调度实体可以是一个进程或由进程组成的控制组。为此，系统时间被分为每周期 1ms（为了方便计算实际值是 1024µs）的序列。在每一个周期中，实体对系统负载的贡献可以由该实体处于可运行状态的时间来计算，可运行状态指正在运行，或在就绪队列等待 CPU。然而，如何描述超过 1ms 时间的负载贡献呢？这可以通过将该周期的负载加上衰减的前一时期负载来表示。若 Li 表示在第 i 个周期内指定实体的负载贡献，则实体的总贡献可以表示为：

L = L0 + L1*y + L2*y2 + L3*y3 + ...

其中 `y (0<y<1)` 是指定的衰减因子。该公式对最近的载荷给予最大的权重，也允许过去的负载以等比递减的方式影响总负载计算。这样计算的优点是，实际上并不需要保留过去每一个周期内的负载贡献值；只需将前一时期的总负载贡献乘以 y 并加上新的 L0 就行了。

在当前代码中，选定的 y^32=0.5（这个 y 是一个小数，然而内核用了一些技巧使计算都使用整数进行）。因此，一个调度实体的负载值在经过32个周期之后，对当前时间负载值的贡献会衰减一半。

现在我们有了计算可运行进程负载的方法，则每个控制组的负载等于其包含的所有可运行进程的负载之和。计算可运行实体造成的负载贡献是容易的，因为调度程序会定期处理这些调度实体。难点在于如何计算不可运行的实体的负载贡献。因为不可运行的进程也能贡献负载，例如一个密码破解程序当前因为缺页（page fault）被阻塞，但是它可能在将来给系统带来很重的负载。不可运行的进程不被调度程序所关心，所以需要一种跟踪不可运行进程的负载贡献的方法。

当然，可以遍历所有的阻塞进程，照常计算衰减负载贡献，并将其添加到总负载中。但这么做代价很大。 因此，相反，3.8 版本的调度程序在每个 cfs_rq（和控制组运行队列）数据结构中维护一个单独的“阻塞负载（blocked load）”，为所有阻塞进程的负载之和。当某个进程阻塞时，从可运行的总负载值中减去其负载，并将其添加到阻塞负载中。阻塞负载按正常方式衰减。当阻塞的进程再次变得可运行时，其（适当衰减的）负载将转移回可运行的负载。因此，仅需要在进程状态转换期间进行计算，就可以跟踪队列负载，而不用遍历所有阻塞进程。

另一个麻烦是节流进程（throttled processes）。若一个进程在在 CFS 带宽控制器（bandwidth controller）下运行，并且已经使用了这一个调度周期内所有分配给他的可用 CPU 时间，则这个进程被称为节流进程。如果一个节流进程仍然希望运行，并且 CPU 也处于空闲状态，调度程器也仍然不会让他运行。因此，节流进程无法增加负载，因此可以在总负载中消除它们的负载。但是，如果让正在等待再次运行的节流进程的负载贡献衰减，会使它们的负载不不合理变小。因此，被节流的进程只需要简单暂停其时间线，不衰减也不计入总负载。

**有什么好处？**

所有这些工作在没有增加开销的情况下，让调度器对每个进程和控制组对系统负载的贡献有了更清晰的认识。更好的统计信息通常是好的，但您可能会想知道这些信息是否对调度器真正有用。

通过更好地了解调度实体的负载贡献我们可以做更多有用的事情。最明显的目标是负载平衡：给系统中的每个 CPU 分配进程，使每个 CPU 的负载大致相同。如果内核知道每个进程对系统负载的贡献，就可以轻松地计算出将该进程迁移到另一个 CPU 的影响。从而带来更准确，更少出错的负载平衡效果。目前已经有了一些[补丁](https://lwn.net/Articles/521272/)利用这种负载跟踪来改善调度器的负载均衡。相信在不久的将来，肯定会有一些相关的东西合并进主线。

需要按调度实体进行负载跟踪的另一个功能是[小任务打包补丁](https://lwn.net/Articles/520857/)。该补丁的目标是将“小”进程收集到少量 CPU 上运行，从而允许关闭系统中的其他处理器。显然，这种收集需要可靠的指标指出哪些进程是“小”的。否则，系统很可能最终处于高度不平衡的状态。

操作系统中的其他子系统也可能使用此信息。例如，CPU 频率和功率调节器（frequency and power governors）应该能够更好地猜测在不久的将来需要多少计算能力。现在有了这个基础设施，很可能会看到更多开发人员使用调度实体的负载统计信息来优化系统行为。虽然我们仍然没有预测未来的魔法水晶球，但至少我们对当前的情况有了更好的了解。
